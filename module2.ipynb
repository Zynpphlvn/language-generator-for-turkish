{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING SENTENCE, PLEASE WAIT...\n",
      "o çiğ tür açmak\n",
      "Sentence Value: 150\n",
      "o bedava gama olmak\n",
      "Sentence Value: 150\n",
      "ben doğal defne açmak\n",
      "Sentence Value: 150\n",
      "ben haberdar çan açmak\n",
      "Sentence Value: 150\n",
      "o geçen hata aşmak\n",
      "Sentence Value: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import random as rnd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from os.path import join\n",
    "from jpype import JClass, getDefaultJVMPath, java, shutdownJVM, startJVM\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ZEMBEREK_PATH: str = join('..', '..', 'bin', 'zemberek-full.jar')\n",
    "\n",
    "    startJVM(\n",
    "            getDefaultJVMPath(),\n",
    "            '-ea',\n",
    "            f'-Djava.class.path={\"C://Users//ASUS//Desktop//cz//distributions//0.17.1//zemberek-full.jar\"}',\n",
    "            convertStrings=False\n",
    "    )\n",
    "   \n",
    "    TurkishMorphology: JClass = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    Paths: JClass = JClass('java.nio.file.Paths')\n",
    "\n",
    "    morphology: TurkishMorphology = TurkishMorphology.createWithDefaults()\n",
    "\n",
    "path = \"C://Users//ASUS//Desktop//cz//1150haber\"\n",
    "fileNames = []\n",
    "wordList = []\n",
    "\n",
    "fileNames = [ subdir+os.path.sep+file for subdir, dirs, files in os.walk(path) for file in files ]\n",
    "\n",
    "tfidfVectorizer = TfidfVectorizer(decode_error='ignore')\n",
    "docTermMatrix = tfidfVectorizer.fit_transform((open(f,encoding=\"utf8\").read() for f in fileNames))\n",
    "\n",
    "wordList = [ word[0] for word in tfidfVectorizer.vocabulary_.items() ]\n",
    "\n",
    "verb_values = []\n",
    "subject_values = []\n",
    "adj_values = []\n",
    "noun_values = []\n",
    "\n",
    "def generate_sentence():\n",
    "    global verb_values\n",
    "    global subject_values\n",
    "    global adj_values\n",
    "    global noun_values\n",
    "\n",
    "    verb_list = []\n",
    "    subject_list = []\n",
    "    adj_list = []\n",
    "    noun_list = []\n",
    "    sentence_sum = 150\n",
    "    \n",
    "    for i in wordList:\n",
    "        sentence: str = i\n",
    "        analysis: java.util.ArrayList = morphology.analyzeSentence(sentence)\n",
    "\n",
    "        results: java.util.ArrayList = (\n",
    "            morphology.disambiguate(sentence, analysis).bestAnalysis()\n",
    "        )\n",
    "        \n",
    "        for j, result in enumerate(results, start=0):\n",
    "            x = (str(result)).split(\":\")[1]\n",
    "            y = x.split(\"]\")[0]\n",
    "\n",
    "            if y == \"Verb\": #Select Verb Data\n",
    "                x = (str(result)).split(\":\")[0]\n",
    "                y = x.split(\"[\")[1]\n",
    "                verb_list.append(y)\n",
    "                verb_list = list(dict.fromkeys(verb_list))\n",
    "            elif y == \"Pron,Pers\": #Select Subject Data\n",
    "                x = (str(result)).split(\":\")[0]\n",
    "                y = x.split(\"[\")[1]\n",
    "                subject_list.append(y)\n",
    "                subject_list = list(dict.fromkeys(subject_list))\n",
    "            elif y == \"Adj\": #Select Adj Data\n",
    "                x = (str(result)).split(\":\")[0]\n",
    "                y = x.split(\"[\")[1]\n",
    "                adj_list.append(y)\n",
    "                adj_list = list(dict.fromkeys(adj_list))\n",
    "            elif y == \"Noun\": #Select Noun Data\n",
    "                x = (str(result)).split(\":\")[0]\n",
    "                y = x.split(\"[\")[1]\n",
    "                noun_list.append(y)\n",
    "                noun_list = list(dict.fromkeys(noun_list))\n",
    "        \n",
    "    if not subject_list:\n",
    "        subject_list.append(\"ben\")\n",
    "        subject_list.append(\"sen\")\n",
    "        subject_list.append(\"o\")\n",
    "        subject_list.append(\"biz\")\n",
    "        subject_list.append(\"siz\")\n",
    "        subject_list.append(\"onlar\")\n",
    "        \n",
    "    verb_values = valueList(verb_list)\n",
    "    subject_values = valueList(subject_list) \n",
    "    adj_values = valueList(adj_list) \n",
    "    noun_values = valueList(noun_list)\n",
    "    \n",
    "    # Create a zip object from two lists\n",
    "    zipbObj = zip(verb_values, verb_list)\n",
    "    # Create a dictionary from zip object\n",
    "    dictOfVerbs = dict(zipbObj)\n",
    "    gen_sentence_list = []\n",
    "    \n",
    "    print(\"GENERATING SENTENCE, PLEASE WAIT...\")\n",
    "#                 gen_verb =  dictOfVerbs[verb_list[verb_values.index(remain_value_verb)]]\n",
    "\n",
    "    \n",
    "    while True:\n",
    "#         gen_verb = rnd.choice(verb_list)\n",
    "        gen_subject = rnd.choice(subject_list)\n",
    "        gen_adj = rnd.choice(adj_list)\n",
    "        gen_noun = rnd.choice(noun_list)\n",
    "        gen_sent_value = subject_values[subject_list.index(gen_subject)] + adj_values[adj_list.index(gen_adj)] + noun_values[noun_list.index(gen_noun)]\n",
    "        remain_value_verb = sentence_sum - gen_sent_value\n",
    "        \n",
    "        try:\n",
    "            gen_verb =  verb_list[verb_values.index(remain_value_verb)]\n",
    "            gen_sent = gen_subject + \" \" + gen_adj + \" \" + gen_noun + \" \" + gen_verb\n",
    "            print(gen_sent + \"\\nSentence Value: \" + str(gen_sent_value + remain_value_verb))\n",
    "            gen_sentence_list.append(gen_sent)\n",
    "            if len(gen_sentence_list) == 5:\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def valueList(word_list):\n",
    "    \n",
    "    letterValue = {'a':1, 'b':2, 'c':3, 'ç':4, 'd':5, 'e':6, 'f':7, 'g':8, 'ğ':9, 'h':10, 'ı':11,\n",
    "                 'i':12, 'j':13, 'k':14, 'l':15, 'm':16, 'n':17, 'o':18, 'ö':19, 'p':20, 'r':21,\n",
    "                 's':22, 'ş':23, 't':24, 'u':25, 'ü':26, 'v':27, 'y':28, 'z':29 }\n",
    "    values = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        total = 0      \n",
    "        for letter in word:\n",
    "            if letter not in letterValue:\n",
    "                continue\n",
    "            total = total + letterValue[letter]\n",
    "            \n",
    "        values.append(total)\n",
    "\n",
    "    return values\n",
    "\n",
    "generate_sentence()\n",
    "\n",
    "shutdownJVM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
